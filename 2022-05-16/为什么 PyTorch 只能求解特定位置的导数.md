不知道你在使用 PyTorch 框架的时候，有没有一个疑惑：PyTorch 框架是怎么实现自动微分的？也就是怎么实现自动求导的？

在这篇文章，我用一个非常简单的函数，来理解一下自动求导的原理。

## 铺垫一下

如果你对这个问题产生了疑惑，那么我相信你已经知道了 PyTorch 会对计算的过程生成一个数据流图，也就是框架会自动记录你的数值计算过程，它会知道你的每一个数值都是怎么来的。如果你对这个不是很了解，推荐你阅读我之前的一篇文章：

传送门：----------------



## 举个例子

用一个非常简单的函数：
$$
\begin{aligned}
&y(x)=2 x^{2} \\
&y^{\prime}(x)=4 x
\end{aligned}
$$
这是一个平平无奇的一元二次函数，并且我也给出了它的导函数。但是呢，如果你让计算机来计算出 y 的导数表达式，这是一件非常困难的事情，因为计算机在硬件的实现上只支持整数和浮点数运算再加上一些逻辑运算。



对了：计算导数的值，一切还是要从导数的定义出发，还记得导数的极限定义式子嘛？就是这个：
$$
y^{\prime}(x)=\lim _{h \rightarrow 0} \frac{y(x+h)-y(x)}{h}
$$
只需要通过导数的定义来计算就可以了！你有没有恍然大悟？那就是为什么 PyTorch 只能计算在某个点上的导数值呢？

式子中的 x 就相当于在使用 `PyTorch` 时候的一个 1 行 1 列大小的 `tensor` 对象，因为 tensor 对象中的值是存在计算机中的，计算机中不可能给一块内存空间标记为一个可变的值 x，所以只要分配了内存空间，那么这个空间的数据就会被解释为一个值。当然了，tensor 对象在初始化的时候，也会自动为它的存储空间赋予一个默认值。所以这就决定了，在计算导数值的时候，只能够通过框架计算出某一个点的导数值。



**具体的做法**

既然 h 是一个趋向于 0 的值，这是极限的思想，但是在计算机中也不可能彻底实现极限的精确值，我们只能试图让 h 等于一个足够小的值来模拟极限的计算过程。

通过刚才的阐述，也知道了 x 并非一个变量，对于计算机来说它只是一个常量。现在我就假设我为 x 赋予了一个初始值是 1。

假设 ==> `x = 1`

取一个 h 的值 ==> `h = 0.001`

那么现在，可以近似的计算出 x = 1 时候的导数值
$$
y^{\prime}(1) \approx \frac{y(1+0.001)-y(1)}{0.001} \\
y^{\prime}(1) \approx \frac{0.004002}{0.001} = 4.002
$$
当 x = 1 的时候，这一点的导数值的真实值为 4

这样一来，我们就得到了一个函数在某个点的近似导数值，当然了，你可以把 h 的取值让它再小一点 : )