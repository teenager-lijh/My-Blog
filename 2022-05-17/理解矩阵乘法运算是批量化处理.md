# 理解矩阵乘法运算是批量化处理

在这篇文章中分享一下我对矩阵和矩阵的乘法在一个视角的理解，我觉得这个理解非常有用，在文章的后边，你会看到这个理解在机器学习中的应用。

## 一个大大的疑惑

在你初学线性代数的时候，不知道你有没有一个这样的疑惑：为什么矩阵和向量的乘法定义成这个样子？为什么矩阵和矩阵的乘法要定义成那个样子？为什么呐？如果你对这篇文章的题目感兴趣，相信你肯定知道了单纯在数值的层面如何计算。



## 运算规则

看到这里，你可能感觉你要对我的这篇文章失望了，在这里我想补充一下这篇文章的完整性，然后再过度到我说的批量化处理的视角来理解。

**向量和向量的乘法**
$$
\begin{aligned}
&a=\left[\begin{array}{lll}
1 & 2 & 3
\end{array}\right] \\
\\
&x=\left[\begin{array}{l}
1 \\
6 \\
9
\end{array}\right]
\end{aligned}
$$
计算：

```python
import numpy as np
a = np.array([1, 2, 3]).reshape(1, -1)
x = np.array([1, 6, 9]).reshape(-1, 1)
print('vector_a : ', a)
print('vector_x : \n', x)

res = a.dot(x)
print('a dot x : ', res)
```

![image-20220517103258568](%E7%90%86%E8%A7%A3%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E8%BF%90%E7%AE%97%E6%98%AF%E6%89%B9%E9%87%8F%E5%8C%96%E5%A4%84%E7%90%86.assets/image-20220517103258568-16527702676842.png)

向量 a 点乘向量 x 的最终结果为 40

要注意：向量 a 在点乘符号的左边，向量 b 在点乘符号的右边，一个行向量和一个列向量做点积的运算规则就是左边的行向量的每个分量和右边的列向量的每个分量分别相乘最后把它们相加起来。

`一个行向量 点乘 一个列向量`

![image-20220517110344096](%E7%90%86%E8%A7%A3%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E8%BF%90%E7%AE%97%E6%98%AF%E6%89%B9%E9%87%8F%E5%8C%96%E5%A4%84%E7%90%86.assets/image-20220517110344096-16527702740175.png)

`a 点乘 b = 1*1 + 2*6 + 3*9 = 40` 

在这里得到了结果 40，

其实：

1. 40 可以看作是一个特殊的向量，这个向量只有一个元素
2. 40 可以看作是一个特殊的矩阵，这个矩阵只有一个元素



**矩阵和向量的乘法**

矩阵和向量的乘法，其实还是遵守了同样的规则：
$$
\begin{aligned}
&A=\left[\begin{array}{lll}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{array}\right] \\
&x=\left[\begin{array}{l}
1 \\
6 \\
9
\end{array}\right]
\\ \\
\end{aligned}
$$
计算：

```python
A = np.array([i for i in range(1, 10)]).reshape(3, 3)
x = np.array([1, 6, 9]).reshape(-1, 1)

res = A.dot(x)
print(res)
```

![image-20220517110618024](%E7%90%86%E8%A7%A3%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E8%BF%90%E7%AE%97%E6%98%AF%E6%89%B9%E9%87%8F%E5%8C%96%E5%A4%84%E7%90%86.assets/image-20220517110618024-16527702773928.png)





如果你把一个向量当作一个整体来看而不是单个的元素来看的话，那其实就是这个样子的一个运算过程：

`第 1 个行向量与列向量做点积 得到 40`

![image-20220517105455879](%E7%90%86%E8%A7%A3%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E8%BF%90%E7%AE%97%E6%98%AF%E6%89%B9%E9%87%8F%E5%8C%96%E5%A4%84%E7%90%86.assets/image-20220517105455879-165277030000326.png)



`第 2 个行向量与列向量做点积 得到 88`

![image-20220517105601799](%E7%90%86%E8%A7%A3%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E8%BF%90%E7%AE%97%E6%98%AF%E6%89%B9%E9%87%8F%E5%8C%96%E5%A4%84%E7%90%86.assets/image-20220517105601799-165277029842423.png)

`第 3 个行向量与列向量做点积 得到 88`

![image-20220517105642436](%E7%90%86%E8%A7%A3%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E8%BF%90%E7%AE%97%E6%98%AF%E6%89%B9%E9%87%8F%E5%8C%96%E5%A4%84%E7%90%86.assets/image-20220517105642436-165277029596620.png)

通过这个过程，你发现了吗？其实矩阵和向量的乘法依然可以看成是向量和向量的运算。

其实：

1. 一个单个的行向量你也可以理解为是一个只有一行数值的特殊的矩阵
2. 一个单个的列向量你也可以理解为只有一列数据的特殊的矩阵



**暂停一下，矩阵是变换**

这个概念非常重要，在这篇文章我先不重点介绍了，但是这个概念对于理解这篇文章后边的概念还蛮重要的。对于这个概念我会在之后单独写一篇文章来说明。不过我在这里简单的说明一下，你还记得正交矩阵吗？记得投影的概念吗？回想一下刚才矩阵和向量的运算，是不是都是左边矩阵的行向量和右边的列向量相乘？

如果左边的矩阵是一个正交矩阵会发生什么？提一下：正交矩阵的每个行向量都是一个单位向量，也就是说正交矩阵的每个行向量的长度（模长）都等于 1。

那么，向量和向量的乘法在几何意义上是怎么样的呢？

我用两个向量分别是 向量a 和 向量b，并且向量a 的长度为 1。双竖线是取模的意思，就是代表一个向量的长度。

![image-20220517112454383](%E7%90%86%E8%A7%A3%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E8%BF%90%E7%AE%97%E6%98%AF%E6%89%B9%E9%87%8F%E5%8C%96%E5%A4%84%E7%90%86.assets/image-20220517112454383-165277029312017.png)



我们先来看看在几何意义上的表示，我现在做一个向量 b 在向量 a 上边的投影向量，那么这个投影向量我使用向量 c 来表示。

![image-20220517112751376](%E7%90%86%E8%A7%A3%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E8%BF%90%E7%AE%97%E6%98%AF%E6%89%B9%E9%87%8F%E5%8C%96%E5%A4%84%E7%90%86.assets/image-20220517112751376-165277029104114.png)



那么向量a 点乘向量b，除了用对应分量相乘再累加得出计算结果，还可以使用下边的计算方法，这个方法在高中的阶段就已经接触过了：
$$
\begin{aligned}
\vec{a} \cdot \vec{b} &=\|\vec{a}\| \cdot\|\vec{b}\| \cdot \cos \theta \\
&=\|\vec{a}\| \cdot(\|\vec{b}\| \cdot \cos \theta) \\
&=\|\vec{b}\| \cdot \cos \theta \\
&=\|\vec{c}\|
\end{aligned}
$$
那就是向量a 点乘 向量b 就可以写成向量 a 的长度乘以向量b 的长度再乘以向量a 和向量b 之间的夹角的 `cos(theta)` 的值。

这时候你看下公式的第二行，我在上边加了一对括号，这表示，我要把这个计算看成是向量b 在向量a 上边的投影。

当然了，你把这个计算看成是向量a 在向量b 上的投影也可以，但是在这里，我们就按照图示的方式来理解这个计算过程。由于向量a 是一个单位向量，所以向量b 乘以 `cos(theta)` 其实向量c 的长度（模长）。

那么这时候回想一下正交矩阵的意义，你是不是就能理解什么叫变换了？是不是就能理解矩阵和向量的乘法在几何上的意义了呢？

正交矩阵的每一个行向量都是单位向量，并且任意两个行向量之间都是正交的，也就是任意两个行向量之间都是垂直的，你想一下三维的笛卡尔坐标系，其中的三个轴是不是就是任意两个轴都是正交的呢？那么，一个正交矩阵其实就以标准的笛卡尔坐标系作为参考系表达出了一个新的坐标系。

这时候真的必须要发挥一下想象力了！那么我们如何得到一个在原坐标系下的一个向量在新的坐标系下的坐标呢？那就是！我们把这个原坐标系下的一个向量（这个向量其实就是坐标点的表示啊！）在新的坐标系下的每一个轴上做投影再求出每个轴上投影点的长度，这样以来，我们就完成了坐标系的转换！你知道了长度，不是就知道了新的坐标系下的每个轴上对应的坐标点的值了嘛？

那么，一个单位行向量点乘一个列向量得出的点积的值，这个值其实就是把这个列向量映射到行向量所表示的轴上的坐标点的值！

如果左边不是一个行向量而是一个矩阵呢？这样是不是就求出来一个列向量在这个矩阵的每个行向量所表示的坐标轴上的投影点的值？如果左边的矩阵是一个正交矩阵，这样就完成了一个坐标系的线性变换！

![image-20220517105642436](%E7%90%86%E8%A7%A3%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E8%BF%90%E7%AE%97%E6%98%AF%E6%89%B9%E9%87%8F%E5%8C%96%E5%A4%84%E7%90%86.assets/image-20220517105642436-165277028593711.png)

这个是刚才的矩阵和向量运算的一张图片，如果左边的矩阵是一个正交矩阵的话（显然这个矩阵并不是正交矩阵，我就不做新的图了），那么右边得出的计算结果 （40， 88， 136）就是坐标点 （1， 6， 9）完成变换后，在新的坐标系下的坐标点的表示。

**矩阵和矩阵的乘法**

**批量化处理！**

这里我们只讨论正交矩阵，其他类型的矩阵也非常类似

如果说一个正交矩阵和一个向量的乘法是把一个列向量（一个列向量其实就是一个坐标点）转化到新的坐标系下，通过矩阵和向量乘法求出这个坐标点在新的坐标系下的坐标点的表示。

这只是一个转化了一个坐标点到新的坐标系下，如果我们有一大堆坐标点想要转化到新的坐标系下应该怎么办？

对！那就是矩阵和矩阵的乘法，来完成**批量化处理**！

值得注意的是，这里的 A 是一个正交矩阵了，为了演示这个内容：

![image-20220517141320345](%E7%90%86%E8%A7%A3%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E8%BF%90%E7%AE%97%E6%98%AF%E6%89%B9%E9%87%8F%E5%8C%96%E5%A4%84%E7%90%86.assets/image-20220517141320345-165277031399229.png)

好！那么现在直接计算 `A 点乘 X` 的值：

**创建矩阵**

我用 Python 的代码创建了和上边表示的等价的矩阵，只不过根号这种操作后的数值在计算机中只能以浮点数的形式表示，所以就变成了下边的样子：

![image-20220517124336819](https://kuku-resources.oss-cn-beijing.aliyuncs.com/images/image-20220517124336819.png)



**验证矩阵 A**

这样就验证了矩阵 A 是单位矩阵，我分别验证了：

1. 每个行向量的长度是否为 1，也就是它自己的点积是否为 1，你可以发现下边的0.99999这样的数值，因为计算机在计算后发生了一点误差，不过这个误差非常小，可以忽略不记
2. 另外验证了任意两个行向量之间是否正交，也就是任意两个行向量之间的点积是否为 0

![image-20220517124509902](%E7%90%86%E8%A7%A3%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E8%BF%90%E7%AE%97%E6%98%AF%E6%89%B9%E9%87%8F%E5%8C%96%E5%A4%84%E7%90%86.assets/image-20220517124509902-165277031698932.png)



**矩阵A 点乘 X**

![image-20220517124720141](%E7%90%86%E8%A7%A3%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E8%BF%90%E7%AE%97%E6%98%AF%E6%89%B9%E9%87%8F%E5%8C%96%E5%A4%84%E7%90%86.assets/image-20220517124720141-165277031938335.png)

`2.02930727e-17 = 0` 这个值差不多是 2 的负 17 次方，那就当作 0 来看就好了 : )

为什么说是批量化处理？现在你会发现，点乘后的结果：

1. 第 1 列的值和第 3 列的值是相等的
2. 第 2 列的值和第 4 列的值是相等的

这是为什么呢？因为 X 矩阵中的值也符合这样的规律，说明 `矩阵A 点乘 矩阵X` 批量化的转化了 X 矩阵中的每个列向量所表示的点到 A 矩阵所表示的坐标系下。那么结果矩阵中的每一个列向量就代表了 X 矩阵中每个列向量变换后的结果向量。



## 在机器学习中的理解

在机器学习中有一个算法叫 PCA，有一篇文章阐述了 PCA 算法的原理，通过 PCA 后，可以用来对数据进行降维处理，啊！这篇文章的篇幅太长了，至于这部分内容，我就放到下一次的文章里了。

关于 PCA 的原理的文章

传送门：

